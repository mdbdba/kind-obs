---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: edge-telemetry-vector
  namespace: flux-system
spec:
  targetNamespace: telemetry
  chart:
    spec:
      chart: vector
      sourceRef:
        kind: HelmRepository
        name: vector
      version: 0.41.0
  interval: 10m0s
  values:
    fullnameOverride: "edge-telemetry"
    service:
      type: ClusterIP
      ports:
        - name: metrics
          port: 9598
          protocol: TCP

    role: "Agent"  # Since this is an edge collector
    logLevel: "info"
    extraVolumes:
      - name: custom-var-log
        hostPath:
          path: /var/log
      - name: custom-docker-containers
        hostPath:
          path: /var/lib/docker/containers
      - name: custom-pod-logs
        hostPath:
          path: /var/log/pods
      - name: vector-data
        emptyDir: {}


    extraVolumeMounts:
      - name: custom-var-log
        mountPath: /var/log
      - name: custom-docker-containers
        mountPath: /var/lib/docker/containers
        readOnly: true
      - name: custom-pod-logs
        mountPath: /var/log/pods
        readOnly: true
      - name: vector-data
        mountPath: /var/lib/vector


    customConfig:
      sources:
        # Your original data sources here, for example:
        logs_source:
          type: "file"
          include:
            - "/var/log/**/*.log" # standard system logs
            - "/var/lib/docker/containers/**/*.log" # container logs
            - "/var/log/pods/**/*.log" # k8s logs
          exclude:
            - "/var/log/containers/edge-telemetry-*_telemetry_vector-*.log"
            - "/var/log/pods/telemetry_edge-telemetry-*_*/vector/*.log"

          ignore_older: 86400  # 1 day in seconds
          max_read_bytes: 10485760  # 10MB
          multiline:
            mode: "halt_before"  # This mode assumes log entries start with specific patterns
            start_pattern: '^\d{4}-\d{2}-\d{2}'  # Assumes logs start with dates like 2025-03-30
            condition_pattern: '^\d{4}-\d{2}-\d{2}'  # Same pattern to identify new log lines
            timeout_ms: 1000  # Max time to wait for additional lines


      transforms:
        prepare_logs:
          type: "remap"
          inputs: ["logs_source"]
          source: |
            # Add metadata
            .host = get_env_var("HOSTNAME") ?? "unknown"
            .timestamp = now()
            .type = "log"
            
            # Extract container info from path if available
            if exists(.file) {
              .log_file = .file
            
              # Make sure .file is string before using it with parse_regex
              if is_string(.file) {
                # Try to extract container info from the path
                container_regex = parse_regex(.file, r'/var/log/containers/(?<pod_name>[^_]+)_(?<namespace>[^_]+)_(?<container_name>[^-]+)-(?<container_id>.+)\.log') 
                if container_regex != null {
                  .kubernetes = {
                    "pod_name": container_regex.pod_name, 
                    "namespace": container_regex.namespace,
                    "container_name": container_regex.container_name,
                    "container_id": container_regex.container_id
                  }
                }
              }
            
              # Remove original file field
              del(.file)
            }
            
            # If message is a JSON string, parse it
            if is_string(.message) {
              # Handle strip_whitespace safely
              cleaned_message = .message
              # Only try to parse if it looks like JSON
              if starts_with(cleaned_message, "{") || starts_with(cleaned_message, "[") {
                parsed = parse_json(cleaned_message) 
                if parsed != null {
                  .message = parsed
                }
              }
            }


      # 3. Sink: Send to Kafka
      sinks:
        # Primary sink: Kafka
        kafka_output:
          type: "kafka"
          inputs: ["prepare_logs"]
          bootstrap_servers: "kafka-kafka-brokers.queuing.svc:9092"
          topic: "telemetry-logs"
          key_field: "host"  # Use host as partition key
          compression: "snappy"
          encoding:
            codec: "json"
          batch:
            max_bytes: 1048576  # 1MB
            timeout_secs: 1
          healthcheck:
            enabled: true

        # Debug sink: Console output to see data flow
        console:
          type: "console"
          inputs: ["prepare_logs"]
          encoding:
            codec: "json"

    replicas: 1
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
